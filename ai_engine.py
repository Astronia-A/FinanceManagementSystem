import os
import time
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. è®¾ç½®æ¨¡å‹ (æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ  timeout æ—¶é—´)
# num_ctx=4096 å¢åŠ ä¸Šä¸‹æ–‡çª—å£ï¼Œé˜²æ­¢æ•°æ®å¤šäº†è®°ä¸ä½
# timeout=300 è®¾ç½®è¶…æ—¶ä¸º 300ç§’ (5åˆ†é’Ÿ)ï¼Œç»™ AI è¶³å¤Ÿçš„æ€è€ƒæ—¶é—´
llm = OllamaLLM(
    model="llama3.2",
    base_url="http://127.0.0.1:11434",
    num_ctx=4096,
    timeout=300
)

embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://127.0.0.1:11434",
)

# å…¨å±€å˜é‡
vector_store = None
DB_PATH = "faiss_index"


def init_knowledge_base(file_path):
    global vector_store
    print(f"ğŸ“‚ åŠ è½½çŸ¥è¯†åº“: {file_path}")
    docs = []
    try:
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        elif file_path.endswith('.txt'):
            try:
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
            except:
                loader = TextLoader(file_path, encoding='gbk')
                docs = loader.load()
        else:
            return
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return

    if not docs: return

    # åˆ‡ç‰‡é€»è¾‘
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
    splits = text_splitter.split_documents(docs)

    print(f"ğŸ§© å…± {len(splits)} ä¸ªç‰‡æ®µï¼Œæ­£åœ¨å»ºç«‹ç´¢å¼•...")

    try:
        batch_size = 10
        vector_store = None
        for i in range(0, len(splits), batch_size):
            batch = splits[i: i + batch_size]
            if vector_store is None:
                vector_store = FAISS.from_documents(batch, embeddings)
            else:
                vector_store.add_documents(batch)
            time.sleep(0.1)  # ç¨å¾®å¿«ä¸€ç‚¹ç‚¹

        vector_store.save_local(DB_PATH)
        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæ¯•ï¼")
    except Exception as e:
        print(f"âŒ å»ºç«‹ç´¢å¼•å¤±è´¥: {e}")


def load_existing_db():
    global vector_store
    if os.path.exists(DB_PATH):
        try:
            vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
            print("âœ… å·²åŠ è½½æ—§çŸ¥è¯†åº“")
        except:
            pass


def get_financial_analysis(data_summary):
    global vector_store
    if not vector_store: load_existing_db()
    if not vector_store: return "âš ï¸ é”™è¯¯ï¼šè¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ å¹¶åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶ï¼"

    retriever = vector_store.as_retriever()

    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡é¡¾é—®ã€‚è¯·åŸºäºã€èƒŒæ™¯çŸ¥è¯†ã€‘åˆ†æã€è´¢åŠ¡æ•°æ®ã€‘ã€‚

    ã€èƒŒæ™¯çŸ¥è¯†ã€‘:
    {context}

    ã€è´¢åŠ¡æ•°æ®ã€‘:
    {input}

    è¯·ç®€æ˜æ‰¼è¦åœ°ç»™å‡ºåˆ†ææ„è§ï¼ˆä¸­æ–‡ï¼‰ï¼š
    """)

    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    try:
        # è¿™é‡Œ invoke å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ï¼Œå·²åœ¨ä¸Šé¢è®¾ç½®äº† timeout
        response = rag_chain.invoke({"input": data_summary})
        return response["answer"]
    except Exception as e:
        return f"åˆ†æä¸­æ–­: {e} (è¯·æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œæˆ–æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ)"