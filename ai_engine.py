import os
import time
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate


embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://127.0.0.1:11434",
)

vector_store = None
DB_PATH = "faiss_index"

def init_knowledge_base(file_path):
    global vector_store
    print(f"ğŸ“‚ åŠ è½½çŸ¥è¯†åº“: {file_path}")
    docs = []
    try:
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        elif file_path.endswith('.txt'):
            try:
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
            except:
                loader = TextLoader(file_path, encoding='gbk')
                docs = loader.load()
        else:
            return
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return

    if not docs: return

    # åˆ‡ç‰‡é€»è¾‘
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
    splits = text_splitter.split_documents(docs)

    print(f"ğŸ§© å…± {len(splits)} ä¸ªç‰‡æ®µï¼Œæ­£åœ¨å»ºç«‹ç´¢å¼•...")

    try:
        batch_size = 10
        vector_store = None
        for i in range(0, len(splits), batch_size):
            batch = splits[i: i + batch_size]
            if vector_store is None:
                vector_store = FAISS.from_documents(batch, embeddings)
            else:
                vector_store.add_documents(batch)
            time.sleep(0.1)  # ç¨å¾®å¿«ä¸€ç‚¹ç‚¹

        vector_store.save_local(DB_PATH)
        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæ¯•ï¼")
    except Exception as e:
        print(f"âŒ å»ºç«‹ç´¢å¼•å¤±è´¥: {e}")

def load_existing_db():
    global vector_store
    if os.path.exists(DB_PATH):
        try:
            vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
            print("âœ… å·²åŠ è½½æ—§çŸ¥è¯†åº“")
        except:
            pass

def get_financial_analysis(data_summary, model_name="llama3.2"):
    """æ”¯æŒä¼ å…¥ model_name"""
    global vector_store
    if not vector_store: load_existing_db()
    if not vector_store: return "âš ï¸ é”™è¯¯ï¼šè¯·å…ˆåŠ è½½çŸ¥è¯†åº“ï¼"

    # åŠ¨æ€åˆ›å»º LLM å¯¹è±¡
    current_llm = OllamaLLM(
        model=model_name,
        base_url="http://127.0.0.1:11434",
        num_ctx=4096,
        timeout=300
    )

    retriever = vector_store.as_retriever()

    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡é¡¾é—®ã€‚è¯·åŸºäºã€èƒŒæ™¯çŸ¥è¯†ã€‘åˆ†æã€è´¢åŠ¡æ•°æ®ã€‘ã€‚

    ã€èƒŒæ™¯çŸ¥è¯†ã€‘:
    {context}

    ã€è´¢åŠ¡æ•°æ®ã€‘:
    {input}

    è¯·ç®€æ˜æ‰¼è¦åœ°ç»™å‡ºåˆ†ææ„è§ï¼ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼‰ï¼š
    """)

    question_answer_chain = create_stuff_documents_chain(current_llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    start_time = time.time()  # å¼€å§‹è®¡æ—¶
    try:
        response = rag_chain.invoke({"input": data_summary})
        end_time = time.time()  # ç»“æŸè®¡æ—¶
        duration = round(end_time - start_time, 2)
        return response["answer"], duration
    except Exception as e:
        return f"åˆ†æé”™è¯¯: {e}", 0

def calculate_similarity_score(text1, text2):
    """
    è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ (ä½™å¼¦ç›¸ä¼¼åº¦)
    è¿”å› 0.0 ~ 1.0 çš„åˆ†å€¼ï¼Œè¶Šé«˜è¶Šå¥½
    """
    if not text1 or not text2:
        return 0.0

    # 1. æŠŠæ–‡å­—å˜æˆå‘é‡ (ä½¿ç”¨å·²åŠ è½½çš„ embeddings æ¨¡å‹)
    # è¿™å°±æ˜¯ RAG çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç°åœ¨æ‹¿æ¥åšè¯„æµ‹
    vec1 = embeddings.embed_query(text1)
    vec2 = embeddings.embed_query(text2)

    # 2. è½¬æ¢ä¸º numpy æ•°ç»„
    v1 = np.array(vec1)
    v2 = np.array(vec2)

    # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼: (A . B) / (|A| * |B|)
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)

    if norm_v1 == 0 or norm_v2 == 0:
        return 0.0

    similarity = dot_product / (norm_v1 * norm_v2)
    return round(float(similarity), 4)  # ä¿ç•™4ä½å°æ•°

# ä¿®æ”¹ get_financial_analysis æ”¯æŒåŠ¨æ€æ¢æ¨¡å‹
def get_financial_analysis_with_model(data_summary, model_name):
    """æ”¯æŒæŒ‡å®šæ¨¡å‹çš„åˆ†æå‡½æ•°"""
    global vector_store
    if not vector_store: load_existing_db()
    if not vector_store: return "çŸ¥è¯†åº“æœªåŠ è½½", 0

    # åŠ¨æ€åˆ›å»ºæŒ‡å®šæ¨¡å‹
    temp_llm = OllamaLLM(
        model=model_name,
        base_url="http://127.0.0.1:11434",
        num_ctx=4096,
        timeout=300
    )

    retriever = vector_store.as_retriever()
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡é¡¾é—®ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†æè´¢åŠ¡çŠ¶å†µï¼š
    ã€èƒŒæ™¯çŸ¥è¯†ã€‘:{context}
    ã€è´¢åŠ¡æ•°æ®ã€‘:{input}
    è¯·ç”¨ä¸­æ–‡ç®€è¦åˆ†æï¼š
    """)
    chain = create_retrieval_chain(retriever, create_stuff_documents_chain(temp_llm, prompt))

    start_time = time.time()
    try:
        res = chain.invoke({"input": data_summary})
        duration = time.time() - start_time
        return res["answer"], round(duration, 2)
    except Exception as e:
        return f"Error: {str(e)}", 0