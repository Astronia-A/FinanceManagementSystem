import os
import time
import numpy as np
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- 1. æ¨¡å‹é…ç½® ---
llm = OllamaLLM(
    model="llama3.2",
    base_url="http://127.0.0.1:11434",
    num_ctx=4096,
    # ã€ä¿®æ”¹1ã€‘è¶…æ—¶æ—¶é—´è®¾ä¸ºæ— é™é•¿æˆ–éå¸¸é•¿ï¼Œé˜²æ­¢åŠ è½½æ¨¡å‹æ—¶æŠ¥é”™
    timeout=600,
    # ã€ä¿®æ”¹2ã€‘å‘Šè¯‰ Ollamaï¼šåŠ è½½è¿›å†…å­˜åï¼Œè‡³å°‘ä¿æŒ 1å°æ—¶(60m) ä¸é€€åœº
    keep_alive="60m"
)

embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://127.0.0.1:11434",
)

vector_store = None
DB_PATH = os.path.join(os.getcwd(), "faiss_index")


def init_knowledge_base(file_path):
    global vector_store
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {file_path}")
    docs = []
    try:
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        elif file_path.endswith('.txt'):
            try:
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
            except:
                loader = TextLoader(file_path, encoding='gbk')
                docs = loader.load()
        else:
            print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            return
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    if not docs: return

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
    splits = text_splitter.split_documents(docs)

    print(f"ğŸ§© åˆ‡åˆ†å®Œæˆï¼Œå…± {len(splits)} ä¸ªç‰‡æ®µã€‚æ­£åœ¨å»ºç«‹ç´¢å¼•...")

    try:
        batch_size = 10
        vector_store = None
        for i in range(0, len(splits), batch_size):
            batch = splits[i: i + batch_size]
            if vector_store is None:
                vector_store = FAISS.from_documents(batch, embeddings)
            else:
                vector_store.add_documents(batch)
            time.sleep(0.1)
        vector_store.save_local(DB_PATH)
        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæ¯•å¹¶å·²ä¿å­˜åˆ° '{DB_PATH}'ï¼")
    except Exception as e:
        print(f"âŒ å»ºç«‹å‘é‡ç´¢å¼•å¤±è´¥: {e}")


def load_existing_db():
    global vector_store
    if os.path.exists(DB_PATH):
        try:
            vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
        except:
            pass


# --- 2. æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šå¼ºåŒ–çš„ Prompt ---
STRONG_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡å®¡è®¡å¸ˆã€‚è¯·åŠ¡å¿…éµå®ˆä»¥ä¸‹æŒ‡ä»¤ï¼š

1. ã€æ ¸å¿ƒä»»åŠ¡ã€‘ï¼šä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯åˆ†æä¸‹é¢çš„ã€è´¢åŠ¡æ•°æ®æ‘˜è¦ã€‘ã€‚
2. ã€è¾…åŠ©å‚è€ƒã€‘ï¼šã€å‚è€ƒçŸ¥è¯†åº“ã€‘ä»…ä½œä¸ºåˆ¤æ–­æ ‡å‡†ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœçŸ¥è¯†åº“è¯´äºæŸä¸å¥½ï¼Œä½ å°±ä¾æ®è¿™ä¸ªæ¥æ‰¹è¯„æ•°æ®ï¼‰ã€‚
3. ã€ç¦æ­¢é¡¹ã€‘ï¼šç»å¯¹ä¸è¦æ€»ç»“æˆ–è¯„ä»·çŸ¥è¯†åº“æœ¬èº«ï¼ä¸è¦è¯´â€œè¿™æ®µæ–‡å­—ä»‹ç»äº†...â€ä¹‹ç±»çš„è¯ã€‚

ã€å‚è€ƒçŸ¥è¯†åº“ã€‘(ç†è®ºä¾æ®):
{context}

ã€è´¢åŠ¡æ•°æ®æ‘˜è¦ã€‘(è¯·é‡ç‚¹åˆ†æè¿™é‡Œçš„æ•°æ®):
{input}

è¯·ç›´æ¥è¾“å‡ºé’ˆå¯¹æ•°æ®çš„åˆ†æç»“è®ºï¼ˆç”¨ä¸­æ–‡ï¼‰ï¼š
"""


def get_financial_analysis(data_summary):
    global vector_store

    # å†…å­˜æ²¡æœ‰å°±è¯»ç¡¬ç›˜
    if vector_store is None:
        load_existing_db()
    if vector_store is None:
        return "âš ï¸ é”™è¯¯ï¼šè¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ å¹¶åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶ï¼"

    retriever = vector_store.as_retriever()
    prompt = ChatPromptTemplate.from_template(STRONG_PROMPT)
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # === ã€ä¿®æ”¹3ã€‘ å¢åŠ â€œè‡ªåŠ¨é‡è¯•â€æœºåˆ¶ ===
    # å¦‚æœç¬¬ä¸€æ¬¡è¿ä¸ä¸Šï¼ˆå› ä¸ºæ¨¡å‹åœ¨åŠ è½½ï¼‰ï¼Œå°±ç­‰2ç§’å†è¯•ä¸€æ¬¡ï¼Œæœ€å¤šè¯•3æ¬¡
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ æ­£åœ¨å°è¯•ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚ AI...")
            response = rag_chain.invoke({"input": data_summary})
            return response["answer"]
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚å¤±è´¥: {error_msg}")

            # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œç­‰å¾…æ¨¡å‹åŠ è½½
            if "Connection" in error_msg or "disconnected" in error_msg:
                time.sleep(2)  # ç­‰2ç§’è®© Ollama å–˜å£æ°”
            else:
                return f"åˆ†æè¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"

    return "âŒ è¿æ¥ Ollama å¤±è´¥ï¼Œè¯·æ£€æŸ¥åå°æœåŠ¡æ˜¯å¦å¼€å¯ï¼Œæˆ–è€…ç”µè„‘æ˜¯å¦å¡é¡¿ã€‚"


def calculate_similarity_score(text1, text2):
    if not text1 or not text2: return 0.0
    try:
        vec1 = np.array(embeddings.embed_query(text1))
        vec2 = np.array(embeddings.embed_query(text2))
        dot = np.dot(vec1, vec2)
        norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)
        if norm == 0: return 0.0
        return round(float(dot / norm), 4)
    except:
        return 0.0


def get_financial_analysis_with_model(data_summary, model_name):
    """ç«æŠ€åœºä½¿ç”¨çš„å‡½æ•°"""
    global vector_store
    if not vector_store: load_existing_db()

    temp_llm = OllamaLLM(
        model=model_name,
        base_url="http://127.0.0.1:11434",
        num_ctx=4096,
        timeout=300
    )

    retriever = vector_store.as_retriever() if vector_store else None

    # ç«æŠ€åœºä¹Ÿä½¿ç”¨å¼ºåŒ–åçš„ Promptï¼Œä¿è¯å…¬å¹³
    prompt = ChatPromptTemplate.from_template(STRONG_PROMPT)

    if retriever:
        chain = create_retrieval_chain(retriever, create_stuff_documents_chain(temp_llm, prompt))
        input_data = {"input": data_summary}
    else:
        return "è¯·å…ˆåŠ è½½çŸ¥è¯†åº“", 0

    start_time = time.time()
    try:
        res = chain.invoke(input_data)
        duration = time.time() - start_time
        return res["answer"], round(duration, 2)
    except Exception as e:
        return f"Error: {str(e)}", 0